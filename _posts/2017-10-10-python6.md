---
title: '刺猬教你量化投资(六):DataFrame数据处理基础'
layout: post
tags: python 量化投资
catalog: 'True'
author: 刺猬偷腥
---
![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1505928760070&di=aae426e9965e0a6b599984935ce7adf0&imgtype=0&src=http%3A%2F%2Fediterupload.eepw.com.cn%2F201411%2F48601416902987.jpg "learning python")

### Pandas进阶知识之数据处理基础

在这一章节，我们将了解Pandas DataFrame的行列数据处理方式，以及学会在茫茫数据中，筛选出我们想要处理的数据。

1.  设置输出效果

想要控制输出的效果，可用set_option()函数，比如：
```
In[1]:
import pandas as pd
pd.set_option('display.max_rows',500) #设置最大行数为500
pd.set_option('display.max_columns',30) #最大列数为20
pd.set_option('precision',8) #显示8为小数
pd.set_option('large_repr','truncate') #truncate是截断的意思，设置对大字符串进行截断
```

2. 对DataFrame求描述性统计

直接举例说明：
```
In[2]:
df=pd.read_excel('learning notebook.xlsx') #导入数据
print '*'*30
print df
print '*'*30
print df.describe(include='all').T #include='all' 表示所有列都求描述性统计，求不出也得求
print '*'*30
print df.info() #获得df的相关信息
print '*'*30
print df.sort_values('price',ascending=False) #根据price降序排序
print '*'*30 
df.sort_values(by=['salary','price'],ascending=[False, True],inplace=True) 
#先按降序对salary进行排列，然后按升序对相同salary的price进行排列，并覆盖原df
#ascending=[False, True]可写成,ascending=[0,1],效果一样
print df
print '*'*30 
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.51      9
4    0.53     14
5    0.59     18
******************************
        count        mean         std  min     25%    50%     75%    max
salary      6  0.49333333  0.06592926  0.4  0.4575  0.495   0.525   0.59
price       6  9.33333333  5.68037557  3.0  5.5000  8.000  12.750  18.00
******************************
<class 'pandas.core.frame.DataFrame'>
Int64Index: 6 entries, 0 to 5
Data columns (total 2 columns):
salary    6 non-null float64
price     6 non-null int64
dtypes: float64(1), int64(1)
memory usage: 144.0 bytes
None
******************************
   salary  price
5    0.59     18
4    0.53     14
3    0.51      9
2    0.48      7
1    0.45      5
0    0.40      3
******************************
   salary  price
5    0.59     18
4    0.53     14
3    0.51      9
2    0.48      7
1    0.45      5
0    0.40      3
```
3. 去重

为了防止求描述性统计时出错，在此之前要先去掉重复的数据。

先创建一个有重复数据的DataFrame：
```
In[3]:
print '*'*30
print df
print '*'*30
df.ix[3,'salary']=0.48
df.ix[5]=[0.53,14]
df
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.51      9
4    0.53     14
5    0.59     18
******************************
Out[3]:
    salary	price
0	0.40	3
1	0.45	5
2	0.48	7
3	0.48	9
4	0.53	14
5	0.53	14
```
去掉完全重复的第六列：
```
In[4]:
print df
print '*'*30
df.ix[3,'salary']=0.48
df.ix[5]=[0.53,14]
print df
print '*'*30
print df.drop_duplicates() 
#默认去除完全重复的行，并保留了所有重复行的第一行，从索引可看出
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.51      9
4    0.53     14
5    0.59     18
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
4    0.53     14
5    0.53     14
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
4    0.53     14
```

还可以有更具体的定义：

```
In[5]:
print '*'*30
print df
print '*'*30
df.ix[3,'salary']=0.48
df.ix[5]=[0.53,14]
print df
print '*'*30
print df.drop_duplicates(subset='salary',keep='last') 
#去掉salary列的重复项，保留最后一行的数据
print '*'*30
print df.drop_duplicates(subset=['salary','price'],keep='last') 
#两列都相同才去重
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.51      9
4    0.53     14
5    0.59     18
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
4    0.53     14
5    0.53     14
******************************
   salary  price
0    0.40      3
1    0.45      5
3    0.48      9
5    0.53     14
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
5    0.53     14
```
如果不去重，只是想看看df里有没有重复的数据，则可用duplicated()函数，将返回布尔值，比如：

```
In[6]:
print '*'*30
df.ix[3,'salary']=0.48
df.ix[5]=[0.53,14]
print df
print '*'*30
print df.duplicated()
#重复则显示True
print df[df.duplicated()]#把重复的数据选出
******************************
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
4    0.53     14
5    0.53     14
******************************
0    False
1    False
2    False
3    False
4    False
5     True
dtype: bool
   salary  price
5    0.53     14
```
4.为df添加列

```
In [7]:

print df
like={0.40:'pig',0.45:'cow',0.48:'dog',0.53:'cat'}
#新建一个字典，将工资水平跟喜欢的动物相对应
df['like']=df.salary.map(float).map(like)
#map(float)意思是将salary列的内容逐一传给map()函数，类型为浮点型
#第二个map(like)表示在like字典里寻找与salary的key一一对应的value
#然后将值赋予新增列like
df
​
   salary  price
0    0.40      3
1    0.45      5
2    0.48      7
3    0.48      9
4    0.53     14
Out[7]:
   salary price like
0	0.40	3	pig
1	0.45	5	cow
2	0.48	7	dog
3	0.48	9	dog
4	0.53	14	cat
```

反过来，上结果倒推出字典也可以：
```
In[8]:
print df
like={0.40:'pig',0.45:'cow',0.48:'dog',0.53:'cat'}
df['like']=df.salary.map(float).map(like)
df
print '*'*30
df1=df[['salary','like']].set_index('salary').to_dict()
#只显示需要的两列，然后设定index，导出成为字典
print df1
print '*'*30
print df1['like']
#只有一个key，不显示key，只显示value
#得到的结果与原始的字典一样

   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
******************************
{'like': {0.45000000000000001: 'cow', 0.47999999999999998: 'dog', 0.40000000000000002: 'pig', 0.53000000000000003: 'cat'}}
******************************
{0.45000000000000001: 'cow', 0.47999999999999998: 'dog', 0.40000000000000002: 'pig', 0.53000000000000003: 'cat'}
```

还可以用apply()函数替代map()函数，效果是一样的：
```
In[9]:
print df
like={0.40:'pig',0.45:'cow',0.48:'dog',0.53:'cat'}
df['like']=df.salary.apply(lambda x:x).map(like)
#将salary的数据导入lambda表达式，然后到like字典里找对应的values
#与map()的区别是，map()可以加字典，而apply()里只能有函数
df
print '*'*30
df1=df[['salary','like']].set_index('salary').to_dict()
print df1
print '*'*30
print df1['like']
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
******************************
{'like': {0.45000000000000001: 'cow', 0.47999999999999998: 'dog', 0.40000000000000002: 'pig', 0.53000000000000003: 'cat'}}
******************************
{0.45000000000000001: 'cow', 0.47999999999999998: 'dog', 0.40000000000000002: 'pig', 0.53000000000000003: 'cat'}
```
对于apply()，lambda x:后面接着的函数，可以将对应的值进行一一处理，比如：
```
In[10]:
print df
df.salary.apply(lambda x:x+1)
​
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[10]:
0    1.40
1    1.45
2    1.48
3    1.48
4    1.53
Name: salary, dtype: float64
```

在apply()中，除了用lambda表达式，还可以直接用自定义函数，比如说：
```
In [11]:
print df
def abc(x):
    return x[:2]
df.like.apply(abc)
​
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[11]:
0    pi
1    co
2    do
3    do
4    ca
Name: like, dtype: object
```
5.  删除列

用drop()或者remove()函数：
```
In [12]:
print df
df.drop('like',axis='columns') #也可以写成axis=1
​
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[12]:
   salary	price
0	0.40	3
1	0.45	5
2	0.48	7
3	0.48	9
4	0.53	14
```
若对两列运作同时删除，则用中括号括起来即可，输入`df.drop(['salary','like'],axis=1)`

 6. 替换序列内容

统计金融数据时，有时需要去极值，以防极端数据对统计数据的错误影响。这时就用到replace(要变的数，变化后的数，inplace=True)函数。
```
In [13]:
print df
df1=df.price.replace(3,np.nan) #把所有value为3的数据变为空值
df1
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[13]:
0   NaN
1     5
2     7
3     9
4    14
Name: price, dtype: float64
```

若要对空值进行填充，可用之前提到的fillna()函数，括号里可以用`method='ffill'`或`method='bfill'`,也可以加入函数，比如求均值则可输入`fillna(df.mean())`,也可以填具体数字或字符串，但需要注意的是，一旦填入字符串，数据类型就不再是浮点型了。

使用replace()一次性将所有空值填充，也可以嵌入函数，比如填充为均值，则输入`df.replace(np.nan, df.mean())`

若要替换多个值，可用列表的形式告知replace()，比如：
```
In [14]:
print df
df1=df.price.replace([3,7],[np.nan,1000]) #把所以value为3的数据变为空值,把7变成1000
df1
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[14]:
0     NaN
1       5
2    1000
3       9
4      14
Name: price, dtype: float64
```

还可以进行条件筛选后再修改值:
```
In [15]:
print df
df[df.salary<0.48]=np.nan
df
   salary  price like
0    0.40      3  pig
1    0.45      5  cow
2    0.48      7  dog
3    0.48      9  dog
4    0.53     14  cat
Out[15]:
    salary	price	like
0	NaN	NaN	NaN
1	NaN	NaN	NaN
2	0.48	7	dog
3	0.48	9	dog
4	0.53	14	cat
```
7.重命名轴名

可以全部一起改：

```
In [16]:
df=pd.DataFrame(np.arange(1,21).reshape((4,5)),index=['dog','cat','pig','cow'],columns=['one','two','three','four','five'])
print df
df.index = df.index.map(str.upper)
#将原数据逐一处理，然后赋予index
print '*'*30
print df
df1=df.rename(index=str.title,columns=str.upper)
#rename()函数可同时定义index和columns
print '*'*30
print df1
     one  two  three  four  five
dog    1    2      3     4     5
cat    6    7      8     9    10
pig   11   12     13    14    15
cow   16   17     18    19    20
******************************
     one  two  three  four  five
DOG    1    2      3     4     5
CAT    6    7      8     9    10
PIG   11   12     13    14    15
COW   16   17     18    19    20
******************************
     ONE  TWO  THREE  FOUR  FIVE
Dog    1    2      3     4     5
Cat    6    7      8     9    10
Pig   11   12     13    14    15
Cow   16   17     18    19    20
```

单独修改也可以：
```
In [17]:

df1
df=pd.DataFrame(np.arange(1,21).reshape((4,5)),index=['dog','cat','pig','cow'],columns=['one','two','three','four','five'])
print df
print '*'*30
df1=df.rename(index={'dog':'bird'},columns={'two':'TWO'})
#修改具体的数据，用字典的格式，把什么变成什么
df1
     one  two  three  four  five
dog    1    2      3     4     5
cat    6    7      8     9    10
pig   11   12     13    14    15
cow   16   17     18    19    20
******************************
Out[17]:
  one	TWO	three	four	five
bird	1	2	3	4	5
cat	6	7	8	9	10
pig	11	12	13	14	15
cow	16	17	18	19	20
```
8. 数据分区间

将连续数据变成分类数据，比如给出一堆个人信息的数据，可以按年龄将其分类，然后再对不同年龄层的人打分赋值，进行差异化处理。

```
In [18]:
ages=[19,22,25,34,38,39,42,46,50]
bins=[18,25,35,60,100]
#也可以写成 bins=range(18,100,10)
a=pd.cut(ages,bins) #创建的区间是左开右闭
print a
print '*'*30
print a.labels
print '*'*30
print a.codes #可以用a.labels做切分，也可以用a.codes，功能一样
#lables和codes都是默认从0开始算，在Python中可以用a.as_unorderedlevels
print '*'*30
print pd.value_counts(a)
print '*'*30
a=pd.cut(ages,bins,right=False) #可以将区间定义为左闭右开
#
print a
print '*'*30
group=['youth','youngadult','middleaged','senior']
b=pd.cut(ages,bins,labels=group) #一步到位指定标签的名字
print b
print '*'*30
print pd.value_counts(b)
[(18, 25], (18, 25], (18, 25], (25, 35], (35, 60], (35, 60], (35, 60], (35, 60], (35, 60]]
Categories (4, object): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]
******************************
[0 0 0 1 2 2 2 2 2]
******************************
[0 0 0 1 2 2 2 2 2]
******************************
(35, 60]     5
(18, 25]     3
(25, 35]     1
(60, 100]    0
dtype: int64
******************************
[[18, 25), [18, 25), [25, 35), [25, 35), [35, 60), [35, 60), [35, 60), [35, 60), [35, 60)]
Categories (4, object): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]
******************************
[youth, youth, youth, youngadult, middleaged, middleaged, middleaged, middleaged, middleaged]
Categories (4, object): [youth < youngadult < middleaged < senior]
******************************
middleaged    5
youth         3
youngadult    1
senior        0
dtype: int64
```

还可以按照一定规则来切分，比如按百分比：
```
In [19]:
print ages
print '*'*30
print pd.qcut(pd.Series(ages),5) 
#按百分比切割，切出5个类别,每个类别都有20%的数据落在其中
print '*'*30 
print pd.value_counts(pd.qcut(ages,5))
print '*'*30 
print pd.qcut(pd.Series(ages),4,labels=group)  #直接加labels
[19, 22, 25, 34, 38, 39, 42, 46, 50]
******************************
0      [19, 23.8]
1      [19, 23.8]
2    (23.8, 34.8]
3    (23.8, 34.8]
4    (34.8, 38.8]
5    (38.8, 43.6]
6    (38.8, 43.6]
7      (43.6, 50]
8      (43.6, 50]
dtype: category
Categories (5, object): [[19, 23.8] < (23.8, 34.8] < (34.8, 38.8] < (38.8, 43.6] < (43.6, 50]]
******************************
(43.6, 50]      2
(38.8, 43.6]    2
(23.8, 34.8]    2
[19, 23.8]      2
(34.8, 38.8]    1
dtype: int64
******************************
0         youth
1         youth
2         youth
3    youngadult
4    youngadult
5    middleaged
6    middleaged
7        senior
8        senior
dtype: category
Categories (4, object): [youth < youngadult < middleaged < senior]
```

9.数据分组

先生成一个能产生随机数的df：
```
In [20]:
df=pd.DataFrame({'key1':['a','a','b','b','c'],
                'key2':['1','2','3','1','2'],
                'data1':np.random.randn(5),
                'data2':np.random.randn(5)}) #随机生成5个符合正态分布的数字
print df
import random
print '*'*30 
print random.randint(1,10) #只生成一个从1到10之间的随机数

      data1     data2 key1 key2
0  0.330967 -1.303240    a    1
1 -0.214045  0.002392    a    2
2 -1.221745 -0.116568    b    3
3 -1.230353 -1.254469    b    1
4  0.931969 -0.429842    c    2
******************************
3
```
现在求当key2是1时，data的最大值。这时候我们就用groupby进行分组：
```
In[21]:
print df
key2_group=df.groupby('key2') #根据key2的值进行分组
print '*'*30 
print key2_group.mean()  #求key2各组的均值
print '*'*30 
print df.groupby('key1').max() #求key1各组的最大值

      data1     data2 key1 key2
0 -0.307291 -0.239133    a    1
1  0.171284 -0.536502    a    2
2 -0.420123 -1.377951    b    3
3  0.414963 -0.494724    b    1
4  0.112269 -0.027871    c    2
******************************
         data1     data2
key2                    
1     0.053836 -0.366929
2     0.141777 -0.282187
3    -0.420123 -1.377951
******************************
         data1     data2 key2
key1                         
a     0.171284 -0.239133    2
b     0.414963 -0.494724    3
c     0.112269 -0.027871    2
```
分组后，数据会存放到内存里，一般不显示出来。如果想知道内存数据的具体信息，则可用以下方式查看：
```
In[22]:
for i,t in df.groupby('key2') :  #i代表key1下的值，t代表这个值所对应的dataframe数据
    print i
    print '*'*30
    print t
    print '*'*30
1
******************************
      data1     data2 key1 key2
0 -0.307291 -0.239133    a    1
3  0.414963 -0.494724    b    1
******************************
2
******************************
      data1     data2 key1 key2
1  0.171284 -0.536502    a    2
4  0.112269 -0.027871    c    2
******************************
3
******************************
      data1     data2 key1 key2
2 -0.420123 -1.377951    b    3
******************************
```
按照两列进行分组：
```
In [23]:
df.groupby(['key1','key2']).sum()
#先按照key1进行分组，在每个组中再按照key2进行分组，然后进行计算求和

Out[23]:
              data1	data2
key1 key2		
a	  1  -0.307291	-0.239133
      2	  0.171284	-0.536502
b	  1   0.414963	-0.494724
      3  -0.420123	-1.377951
c     2	  0.112269	-0.027871
```

只对某项数据按照分组来求值也不难实现：
```
In [24]:
df1=df['data1'].groupby(df['key1'])
df1.sum()

Out[24]:
      key1
a    0.856037
b   -0.160871
c   -0.410777
Name: data1, dtype: float64
```

数据太多看不来，或者只想看第一行数据：
```
In [25]:
df.groupby('key1').head(1) #对key1按组分类，显示每组第一行的数据

Out[25]:
      data1	    data2  	key1	key2
0	-0.257636	1.858826	a	1
2	-0.256259	-0.442108	b	3
4	-0.410777	0.096867	c	2
```

组内分别排好序：
```
In [26]:

对key1按组分类，并从小到大排序
df.groupby('key1').apply(lambda x:x.sort_values(by='data1'))#对key1按组分类，并从小到大排序
Out[26]:
data1	data2	key1	key2
key1					
a	0	-0.257636	1.858826	a	1
1	1.113673	-1.062081	a	1
b	2	-0.256259	-0.442108	b	3
3	0.095389	-0.408265	b	1
c	4	-0.410777	0.096867	c	2
```

若要取分组后每一组的最小值：
```
In [27]:
df.groupby('key1').apply(lambda x:x.sort_values(by='data1').head(1))
#要取排序后的第一行，可以在函数中定义

Out[27]:
        data1	data2      	key1	key2
key1					
a	0	-0.257636	1.858826	a	1
b	2	-0.256259	-0.442108	b	3
c	4	-0.410777	0.096867	c	2
```

10. 插入日期

date_range()函数可以创建日期序列，括号里可以作以下定义：
* start: 开始日期
* end: 结束日期
* periods：日期范围内插入多少个日期
* freq：频率，默认为D，表示1天，表示3天，可输入'3D'
* normalize：把日期标准化到午夜00：00：00

```
In [28]:
df=pd.date_range('20171010',periods=8) #生成8个日期，到2017-10-17
df
Out[28]:
DatetimeIndex(['2017-10-10', '2017-10-11', '2017-10-12', '2017-10-13',
               '2017-10-14', '2017-10-15', '2017-10-16', '2017-10-17'],
              dtype='datetime64[ns]', freq='D')
```

增加定义的参数值再试试：
```
In [29]:
date=pd.date_range('20171010',periods=5,freq='3D') #生成8个日期，到2017-10-22
df=pd.DataFrame(np.random.randn(5,4),index=date,columns=list('abcd'))
df
Out[29]:
                a	b	c	d
2017-10-10	-1.28148698	0.18041147	1.41104298	-1.55570541
2017-10-13	-0.73420162	0.18166044	-1.16679244	2.02844938
2017-10-16	0.02265466	-0.03236252	-0.91341665	-0.66593707
2017-10-19	-0.80841995	-0.48025269	-0.42695733	-0.85956796
2017-10-22	-0.90206823	-0.83091302	-0.56194238	-0.63309995
```

11. 切片和筛选

回忆以下之前介绍过的几种选择行列的方法：
* 方法一：df[0:3]
* 方法二：df.loc[‘20171010’：‘20171016’，‘b’]，只跟标签，设定的范围前后都包
* 方法三：df.iloc[0:2,0:2]，只跟索引，第几行第几列
* 方法四：df.ix()，既跟位置，也跟标签
* 方法五：返回条件判断为True的行，比如df[df.a>0]

现在我们要查看一下哪些行存在我们想要的数据，可以用isin()函数：
```
In [30]:
df1=df.copy() #将DF强制复制到df1
df1['e']=['1','2','3','1','2']
print df1
print '*'*30 
print df1.e.isin(['1','3']) #判断是否存在1和3这两个values
print '*'*30 
df1[df1.e.isin(['1','3'])] #筛选出返回True的行
                     a           b           c           d  e
2017-10-10 -1.28148698  0.18041147  1.41104298 -1.55570541  1
2017-10-13 -0.73420162  0.18166044 -1.16679244  2.02844938  2
2017-10-16  0.02265466 -0.03236252 -0.91341665 -0.66593707  3
2017-10-19 -0.80841995 -0.48025269 -0.42695733 -0.85956796  1
2017-10-22 -0.90206823 -0.83091302 -0.56194238 -0.63309995  2
******************************
2017-10-10     True
2017-10-13    False
2017-10-16     True
2017-10-19     True
2017-10-22    False
Freq: 3D, Name: e, dtype: bool
******************************
Out[30]:
                a	b	c	d	e
2017-10-10	-1.28148698	0.18041147	1.41104298	-1.55570541	1
2017-10-16	0.02265466	-0.03236252	-0.91341665	-0.66593707	3
2017-10-19	-0.80841995	-0.48025269	-0.42695733	-0.85956796	1

```
isin()的好处是可以筛选字符串，而方法四只能判断数字运算结果。

对于字符串，还可以用str.contains()做部分筛选：

```
In [31]:
df1=df.copy() #将DF强制复制到df1
df1['e']=['12','22','5','561','342']
print df1
print '*'*30 
df1[df1.e.str.contains('1|4')] #筛选出与部分字符串相配合的行，|表示并集的意思，有1或者4都切出来。
#如果用&，则表示同时配合1和4才行
                     a           b           c           d    e
2017-10-10 -1.28148698  0.18041147  1.41104298 -1.55570541   12
2017-10-13 -0.73420162  0.18166044 -1.16679244  2.02844938   22
2017-10-16  0.02265466 -0.03236252 -0.91341665 -0.66593707    5
2017-10-19 -0.80841995 -0.48025269 -0.42695733 -0.85956796  561
2017-10-22 -0.90206823 -0.83091302 -0.56194238 -0.63309995  342
******************************
Out[31]:
                a	b	c	d	e
2017-10-10	-1.28148698	0.18041147	1.41104298	-1.55570541	12
2017-10-19	-0.80841995	-0.48025269	-0.42695733	-0.85956796	561
2017-10-22	-0.90206823	-0.83091302	-0.56194238	-0.63309995	342

```

若要选出不匹配的所在行，不能用isnotin()，而是在df.e.isin()前面加个销魂的波浪线，~是取反的意思。
```
In [32]:
df1=df.copy() #将DF强制复制到df1
df1['e']=['12','14','5','561','1342']
print df1
print '*'*30 
df1[~df1.e.str.contains('1','4')] #筛选出与部分字符串相配合的行，|表示并集的意思，有1或者4都切出来
                     a           b           c           d     e
2017-10-10 -1.28148698  0.18041147  1.41104298 -1.55570541    12
2017-10-13 -0.73420162  0.18166044 -1.16679244  2.02844938    14
2017-10-16  0.02265466 -0.03236252 -0.91341665 -0.66593707     5
2017-10-19 -0.80841995 -0.48025269 -0.42695733 -0.85956796   561
2017-10-22 -0.90206823 -0.83091302 -0.56194238 -0.63309995  1342
******************************
Out[32]:
a	b	c	d	e
2017-10-16	0.02265466	-0.03236252	-0.91341665	-0.66593707	5
```

除了对单个值进行判断，我们还能用query()函数对不同的列值做比较，然后返回结果为真的行：
```
In [33]:
df1=df.copy() #将DF强制复制到df1
df1['e']=['12','14','5','561','1342']
print df1
print '*'*30 
df1.query('a>c') #筛选出a大于c的行
#筛选条件可以是并集或者合集，比如'a>0 & a<1' 或者'a>b |a<c'

                     a           b           c           d     e
2017-10-10 -1.28148698  0.18041147  1.41104298 -1.55570541    12
2017-10-13 -0.73420162  0.18166044 -1.16679244  2.02844938    14
2017-10-16  0.02265466 -0.03236252 -0.91341665 -0.66593707     5
2017-10-19 -0.80841995 -0.48025269 -0.42695733 -0.85956796   561
2017-10-22 -0.90206823 -0.83091302 -0.56194238 -0.63309995  1342
******************************
Out[33]:
            a	b	c	d	e
2017-10-13	-0.73420162	0.18166044	-1.16679244	2.02844938	14
2017-10-16	0.02265466	-0.03236252	-0.91341665	-0.66593707	5
```
 
还可以根据列名来进行筛选，用filter()函数：
```
In [34]:
print df
print '*'*30 
df.filter(like='data') #筛选出列名中含有data的列
        data1       data2 key1 key2
0 -0.86657019 -1.00867943    a    1
1  0.02747153 -1.74926362    a    1
2 -1.47649846  0.07885124    b    3
3 -0.20667256 -0.93493089    b    1
4 -0.14481133  0.71316842    c    2
******************************
Out[34]:
        data1	data2
0	-0.86657019	-1.00867943
1	0.02747153	-1.74926362
2	-1.47649846	0.07885124
3	-0.20667256	-0.93493089
4	-0.14481133	0.71316842
```

### 结语

至此，我们已对Pandas DataFrame的行列数据处理有所熟悉。但我们面对3000多只股票的数据时，这些方法将大大提高信息处理的效率。下一章我们将学习更多DataFrame数据处理的方式，敬请期待。



<br/><br/>

刺猬偷腥<br/>
2017年10月10日<br/><br/>
***<br/>
to be continued.